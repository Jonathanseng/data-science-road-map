{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3401a76f-5c53-4190-b629-a934069b1cc2",
   "metadata": {},
   "source": [
    "# Machine Learning with Python - Logistic Regression\n",
    "The basics of classification with linear models\n",
    "\n",
    "### gradient descent and sklearn \n",
    "The most popular framework for fiting popular small data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd3364-d1df-4b98-b55e-6aa8644da473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#read in our titanic data\n",
    "df_og = pd.read_csv('data/train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f91a6-50cc-43f2-a811-e257f93bf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data set into train and test sets remove any non-numeric columns for the example\n",
    "X, y = df_og.drop(columns=['PassengerId','Name','Ticket','Cabin','Embarked','Survived']), df_og['Survived']\n",
    "X = X.replace({'male': 0, 'female': 1}).fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "print('size of X_train') \n",
    "print(X_train.shape)\n",
    "print('size of X_test')\n",
    "print(X_test.shape)\n",
    "print('size of y_train') \n",
    "print(y_train.shape)\n",
    "print('size of y_test')\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8657206-0437-4be1-bc9d-392d6f2f8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f53e0-7072-4a4f-8919-958d27fe3805",
   "metadata": {},
   "source": [
    "Let's implement logistic regression from scratch. We will use a modified example from [this blog post.](https://medium.com/@IwriteDSblog/gradient-descent-for-logistics-regression-in-python-18e033775082)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20c283-d070-41e7-a006-ecd143147251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXvector(X):\n",
    "    \"\"\" Taking the original independent variables matrix and add a row of 1 which corresponds to x_0\n",
    "        Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: the matrix that contains all the values in the dataset, not include the outcomes variables. \n",
    "    \"\"\"    \n",
    "    vectorX = np.c_[np.ones((len(X), 1)), X]\n",
    "    return vectorX\n",
    "\n",
    "def beta_init(X):\n",
    "    \"\"\" Generate an initial value of vector of betas from the original independent variables matrix\n",
    "         Parameters:\n",
    "          X:  independent variables matrix\n",
    "        Return value: a vector of beta filled with initial guess\n",
    "    \"\"\"\n",
    "    beta = np.random.randn((X.shape[1])+1, 1)\n",
    "    return beta\n",
    "\n",
    "def sigmoid_function(X):\n",
    "    \"\"\" Calculate the sigmoid value of the inputs\n",
    "         Parameters:\n",
    "          X:  values\n",
    "        Return value: the sigmoid value\n",
    "    \"\"\"\n",
    "    return 1/(1+math.e**(-X))\n",
    "\n",
    "def Logistics_Regression(X, y, learningrate, iterations):\n",
    "    \"\"\" Find the Logistics regression model for the data set\n",
    "         Parameters:\n",
    "          X: independent variables matrix\n",
    "          y: dependent variables matrix\n",
    "          learningrate: learningrate of Gradient Descent\n",
    "          iterations: the number of iterations\n",
    "        Return value: the final beta vector and the plot of cost function\n",
    "    \"\"\"\n",
    "    y_new = y.values.reshape((len(y), 1))   \n",
    "    cost_lst = []\n",
    "    vectorX = generateXvector(X)\n",
    "    beta = beta_init(X)\n",
    "    m = len(X)\n",
    "    for i in range(iterations):\n",
    "        gradients = 2/m * vectorX.T.dot(sigmoid_function(vectorX.dot(beta)) - y_new)\n",
    "        beta = beta - learningrate * gradients\n",
    "        y_pred = sigmoid_function(vectorX.dot(beta))\n",
    "        cost_value = - np.sum(np.dot(y_new.T,np.log(y_pred)+ np.dot((1-y_new).T,np.log(1-y_pred)))) /(len(y_pred))\n",
    "     #Calculate the loss for each training instance\n",
    "        cost_lst.append(cost_value)\n",
    "    plt.plot(np.arange(1,iterations),cost_lst[1:], color = 'red')\n",
    "    plt.title('Cost function Graph')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706222e1-97af-4c08-8807-b0c8949bc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = Logistics_Regression(X_train, y_train, 0.01, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81214b55-2359-4ec9-9e13-a5edc8743fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c559bfe-759d-40bb-aef0-7d9832409456",
   "metadata": {},
   "source": [
    "Much easier, let's just use the sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d334185-307f-4511-921e-688fb0f10f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34a1dd-7e7e-4da4-8a6f-89c7397d0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0, penalty = 'none')\n",
    "classifier.fit(X_train, y_train)\n",
    "classifier.intercept_, classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c265a-62e3-471b-9fff-64f9ea1374f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_train)\n",
    "\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55996c7-e23f-4397-acad-c5a836af55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83adc7f-75c4-4f28-adbd-c8e624a007c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e444aff-141b-4b8e-8b89-ae7b0c08c1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
